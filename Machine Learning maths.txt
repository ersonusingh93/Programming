Linear Algebra for Machine Learning and Data Science:

Week 1: Systems of Linear Equations

Lesson 1: Systems of Linear equations: two variables

Machine learning motivation
Systems of sentences
Systems of equations
Systems of equations as lines
A geometric notion of singularity
Singular vs nonsingular matrices
Linear dependence and independence
The determinant
Lesson 2: Systems of Linear Equations: three variables

Systems of equations (3×3)
Singular vs non-singular (3×3)
Systems of equations as planes (3×3)
Linear dependence and independence (3×3)
The determinant (3×3)
Week 2: Solving systems of Linear Equations
Lesson 1: Solving systems of Linear Equations: Elimination

Machine learning motivation
Solving non-singular systems of linear equations
Solving singular systems of linear equations
Solving systems of equations with more variables
Matrix row-reduction
Row operations that preserve singularity
Lesson 2: Solving systems of Linear Equations: Row Echelon Form and Rank

The rank of a matrix
The rank of a matrix in general
Row echelon form
Row echelon form in general
Reduced row echelon form
Week 3: Vectors and Linear Transformations
Lesson 1: Vectors

Norm of a vector
Sum and difference of vectors
Distance between vectors
Multiplying a vector by a scalar
The dot product
Geometric Dot Product
Multiplying a matrix by a vector
Lab: Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors
Lesson 2: Linear transformations

Matrices as linear transformations
Linear transformations as matrices
Matrix multiplication
The identity matrix
Matrix inverse
Which matrices have an inverse?
Neural networks and matrices
Week 4: Determinants and Eigenvectors
Lesson 1: Determinants In-depth

Machine Learning Motivation
Singularity and rank of linear transformation
Determinant as an area
Determinant of a product
Determinants of inverses
Lesson 2: Eigenvalues and Eigenvectors

Bases in Linear Algebra
Span in Linear Algebra
Interactive visualization: Linear Span
Eigenbases
Eigenvalues and eigenvectors


Calculus for Machine Learning and Data Science:

Week 1: Functions of one variable: Derivative and optimization
Lesson 1: Derivatives

Example to motivate derivatives: Speedometer
Derivative of common functions (c, x, x^2, 1/x)
Meaning of e and the derivative of e^x
Derivative of log x
Existence of derivatives
Properties of derivative
Lesson 2: Optimization with derivatives

Video 1: Intro to optimization: Temperature example

Video 2: Optimizing cost functions in ML: Squared loss

Video 3: Optimizing cost functions in ML: Log loss

Week 2: Functions of two or more variables: Gradients and gradient descent
Lesson 1: Gradients and optimization

Intro to gradients
Example to motivate gradients: Temperature
Gradient notation
Optimization using slope method: Linear regression
Lesson 2: Gradient Descent

Optimization using gradient descent: 1 variable
Optimization using gradient descent: 2 variable 
Gradient descent for linear regression
Week 3: Optimization in Neural Networks and Newton’s method
Lesson 1: Optimization in Neural Networks

Perceptron with no activation and squared loss (linear regression)
Perceptron with sigmoid activation and log loss (classification)
Two-layer neural network with sigmoid activation and log loss
Mathematics of Backpropagation
Lesson 2: Beyond Gradient Descent: Newton’s Method

Root finding with Newton’s method
Adapting Newton’s method for optimization
Second derivatives and Hessians
Multivariate Newton’s method

Probability & Statistics for Machine Learning & Data Science:

Week 1: Introduction to probability and random variables
Lesson 1: Introduction to probability

Concept of probability: repeated random trials
Conditional probability and independence 
Discriminative learning and conditional probability
Bayes theorem 
Lesson 2: Random variables

Random variables
Cumulative distribution function
Discrete random variables: Bernoulli distribution
Discrete random variables: Binomial distribution
Probability mass function
Continuous random variables: Uniform distribution 
Continuous random variables: Gaussian distribution
Continuous random variables: Chi squared distribution
Probability distribution function
Week 2: Describing distributions and random vectors
Lesson 1: Describing distributions

Measures of central tendency: mean, median, mode
Expected values
Quantiles and box-plots 
Measures of dispersion: variance, standard deviation
Lesson 2: Random vectors

Joint distributions
Marginal and conditional distributions
Independence
Measures of relatedness: covariance
Multivariate normal distribution
Week 3: Introduction to statistics
Lesson 1: Sampling and point estimates

Population vs. sample 
Describing samples: sample proportion and sample mean
Distribution of sample mean and proportion: Central Limit Theorem 
Point estimates
Biased vs Unbiased estimates 
Lesson 2: Maximum likelihood estimation

ML motivation example: Linear Discriminant Analysis
Likelihood
Intuition behind maximum likelihood estimation
MLE: How to get the maximum using calculus
Lesson 3: Bayesian statistics 

ML motivation example: Naive Bayes
Frequentist vs. Bayesian statistics
A priori/ a posteriori distributions
Bayesian estimators: posterior mean, posterior median, MAP
Week 4: Interval statistics and Hypothesis testing
Lesson 1: Confidence intervals

Margin of error
Interval estimation
Confidence Interval for mean of population
CI for parameters in linear regression
Prediction Interval
Lesson 2: Hypothesis testing

ML Motivation: AB Testing
Criminal trial
Two types of errors
Test for proportion and means
Two sample inference for difference between groups 
ANOVA
Power of a test